---
title: "COMP 755 Final Project"
author: "Kevin O'Connor"
date: "11/8/2018"
output:
  pdf_document: default
  html_document: default
---

In this document, we apply three different classification methods on The Cancer Genome Atlas (TCGA) Breast Cancer data. Specifically, we will use gene expression levels to predict breast cancer subtype which falls into one of five different groups: Luminal A, Luminal B, Basal, Normal, and HER2. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries and directories.
library(dplyr)
library(knitr)
library(R.utils)
library(limma)

data.dir <- file.path("/Users/kevinoconnor/Documents/Research/DCM/Data")
home.dir <- file.path("/Users/kevinoconnor/Documents/School/COMP_755")
sourceDirectory(home.dir)

# Reading data.
if(!(exists("SeqData") & exists("sampleTab"))){
  SeqData   <- read.delim(file.path(data.dir, "BRCA_1201_FINAL_Cluster.txt") , 
                          header=F)
  sampleTab <- read.delim(file.path(data.dir, "BRCA_1218_pam50scores_FINAL.txt"))
}
data   <- SeqData[-c(1:5),-(1:3)] 
SamBar <- SeqData[,-c(1:3)]
samTab <- filter(sampleTab, Use=="YES")
gene.names <- SeqData[-c(1:5), 1]
rownames(data) <- gene.names

# Match barcodes in both datasets.
SamBar0 <- unlist(lapply(SamBar[1,], as.character))
m       <- match(SamBar0, as.character(samTab$Barcode))
samTab0 <- samTab[m, ]
dataM0  <- apply(data, 2, as.numeric)
rownames(dataM0) <- as.vector(gene.names)

# Filter and transform data.
dataM <- filter_data(dataM0, min.var=100, max.var=10000)

# Standardize columns.
stdize <- function(gene){
  # Make zero mean.
  gene <- gene - mean(gene)
  # Make sum of squares one.
  gene <- gene/sqrt(sum(gene^2))
  return(gene)
}

dataM.std <- dataM
for(c in 1:ncol(dataM)){
  dataM.std[,c] <- stdize(dataM[,c])
}

# Create matrix for each group.
tar <- samTab0$Call
wb  <- which(tar=="Basal")
wla <- which(tar=="LumA")
wlb <- which(tar=="LumB")
wh <- which(tar=="Her2")
wn <- which(tar=="Normal")

dat.luma   <- dataM.std[, wla]
dat.lumb   <- dataM.std[, wlb]
dat.basal  <- dataM.std[, wb]
dat.her2   <- dataM.std[, wh]
dat.normal <- dataM.std[, wn]

# Save separate data matrices for use in python.
write.table(dat.luma, file=file.path(home.dir, "luma.csv"), row.names=FALSE, col.names=FALSE, sep=",")
write.table(dat.lumb, file=file.path(home.dir, "lumb.csv"), row.names=FALSE, col.names=FALSE, sep=",")
write.table(dat.basal, file=file.path(home.dir, "basal.csv"), row.names=FALSE, col.names=FALSE, sep=",")
write.table(dat.her2, file=file.path(home.dir, "her2.csv"), row.names=FALSE, col.names=FALSE, sep=",")
write.table(dat.normal, file=file.path(home.dir, "normal.csv"), row.names=FALSE, col.names=FALSE, sep=",")
```

After reading in the data and removing genes with extremely small or large variance, we are left with p=`r nrow(dat.luma)` variables and sample sizes for each group given by

```{r sample_size}
kable(data.frame(group=c("Luminal A", "Luminal B", "Basal", "Normal", "HER2"), sample.size=c(ncol(dat.luma), ncol(dat.lumb), ncol(dat.basal), ncol(dat.normal), ncol(dat.her2))))
```

Yielding a total sample size of n=`r ncol(dat.luma)+ncol(dat.lumb)+ncol(dat.basal)+ncol(dat.normal)+ncol(dat.her2)`. Note that we have normalized the columns of each dataset.

Quickly, we will split the data up into a training and test sets.
```{r split_data}
dat <- cbind(dat.luma, dat.lumb, dat.basal, dat.her2, dat.normal)
labels <- c(rep(1, ncol(dat.luma)),
            rep(2, ncol(dat.lumb)),
            rep(3, ncol(dat.basal)),
            rep(4, ncol(dat.her2)),
            rep(5, ncol(dat.normal)))
n <- ncol(dat)
p <- nrow(dat)
train.inds <- sample.int(n, round(0.8*n))
dat.train <- dat[,train.inds]
dat.test <- dat[,-train.inds]
y.train <- labels[train.inds]
y.test <- labels[-train.inds]
```

# Multinomial Logistic Regression

First we will perform logistic regression to try and predict the subgroup which a given set of gene expression values belongs to. We can write our model as
\[ \mathbb{P}(y_i = k | X_i, \beta) = \frac{\exp\left\{X_i \beta_k\right\}}{\sum_{k'} \exp\left\{X_i \beta_{k'}\right\}}\]
Which gives a negative log-likelihood of
\[ \mathcal{NLL}(\beta) = \sum\limits_{i=1}^n -X_i \beta_k - \sum\limits_{i=1}^n \log\bigg(\sum\limits_{k'} \exp\left\{X_i \beta_{k'}\right\}\bigg) \]

```{r neg_log_lkhd}
neg_log_likelihood <- function(beta, X){
  # Returns negative log-likelihood of the logistic regression model.
  #
  # Args:
  #  -beta: p by K matrix where the k'th column gives beta_k as in the equation
  #    above.
  #  -X: p by n data matrix.
  # Returns:
  #  A K by n matrix giving log-likelihood of each point and class.
  require(matrixStats)
  
  return(-t(beta) %*% X - sapply(1:ncol(beta), function(i){logSumExp(t(beta[,i]) %*% X)}))
}

get_log_likelihood_w_pen <- function(beta, X, y, pen_type="l1", lambda=0){
  # Returns penalized log-likelihood of the logistic regression model.
  #
  # Args:
  #  -beta: p by K matrix where the k'th column gives beta_k as in the equation
  #    above.
  #  -X: p by n data matrix.
  #  -y: n by 1 vector giving the labels of the data.
  #  -pen_type: Either "l1" or "l2" corresponding to the type of penalty term.
  #  -lambda: Penalty parameter.
  # Returns:
  #  A number corresponding to the penalized negative log-likelihood.
  
  nll.mat <- neg_log_likelihood(beta, X)
  # Now select only nll for correct classes.
  ind <- 1; nll <- c()
  for(label in y){
    nll <- c(nll, nll.mat[label, ind])
  }
  nll <- sum(nll)
  
  if(pen_type == "l1"){
    pen <- lambda*sum(abs(beta))
  } else if(pen_type == "l2"){
    pen <- lambda*sum(beta^2)
  } else {
    print("ERROR: Unknown penalty type!")
    return(NaN)
  }
  
  return(nll + pen)
}
```

Next we will need an implementation of gradient descent to use for fitting.

```{r grad_desc}
compute_grad <- function(beta, X, y, pen_type="l1", lambda, delta=1e-4){
  # Initialize matrix of gradients.
  grad_mat <- beta
  # Numerically compute gradients of each element of beta.
  for (i in nrow(beta)){
    for (j in ncol(beta)){
      l_beta <- r_beta <- beta
      l_beta[i,j] - delta/2
      r_beta[i,j] + delta/2
      grad_mat[i,j] <- (get_log_likelihood_w_pen(r_beta, X, y, pen_type, lambda) - get_log_likelihood_w_pen(l_beta, X, y, pen_type, lambda))/delta
    }
  }
  return(grad_mat)
}

grad_descent <- function(init_beta, 
                         X, 
                         y, 
                         pen_type="l1", 
                         lambda=0, 
                         delta=1e-4, 
                         learning_rate=1e-2, 
                         max.iter=10){
  # Initialize beta.
  beta <- init_beta
  # Iteratively compute gradient and update beta.
  for(i in 1:max.iter){
    print(paste0("Completed ", i, " iterations..."))
    grad_mat <- compute_grad(beta, X, y, pen_type, lambda, delta)  
    # Update beta.
    beta <- beta - learning_rate*grad_mat
  }
  return(beta)
}

predict_class <- function(beta, X){
  # beta is p by K
  # X is p by n.
  predictions <- sapply(1:ncol(X), function(c){
    return(which.max(-t(beta) %*% X[,c]))
  })
  return(predictions)
}
```

Ok, now we can fit a multinomial logit model to our data.

```{r fit_multlogit}
init_beta <- matrix(c(0), nrow=p, ncol=n)
fitted.beta <- grad_descent(init_beta, dat.train, y.train, max.iter=10)
predicted_ys_train <- predict_class(fitted.beta, dat.train)
predicted_ys_test <- predict_class(fitted.beta, dat.test)
print(paste0("Train accuracy: ", mean(predicted_ys_train == y.train)))
print(paste0("Test accuracy: ", mean(predicted_ys_test == y.test)))
```

# Clustering
In this section, we will use an unsupervised algorithm to cluster the data. We hope that the clusters will correspond to subgroups of the samples. In order to measure the performance, we will compute the purity.

```{r kmeans}
cluster.kmeans <- kmeans(t(dat), 5)

purity <- function(clusters, labels){
  # Takes in a vector of clusters and vector of labels and computes the purity
  # as defined in the paper. This function assumes that the i'th element of 
  # clusters and i'th element of labels correspond to the same data point.
  cluster.labels <- c()
  n.k <- c()
  k <- length(unique(clusters))
  n <- length(clusters)
  
  for(i in 1:k){
    # Get label for each cluster.
    labels.i <- labels[which(clusters==i)]
    max.label.i <- which.max(tabulate(labels.i))
    cluster.labels <- c(cluster.labels, max.label.i)
    
    # Get count of maximal label.
    n.k <- c(n.k, length(which(labels.i==max.label.i)))
  }
  
  return(list("cluster.labels"=cluster.labels, "purity"=sum(n.k/n)))
}

clusters <- cluster.kmeans$cluster
pur.out <- purity(clusters, labels)
pur <- pur.out$"purity"
cluster.labels <- pur.out$"cluster.labels"

print(paste0("Cluster 1 size: ", length(which(clusters==1)), ", label: ", cluster.labels[1]))
print(paste0("Cluster 2 size: ", length(which(clusters==2)), ", label: ", cluster.labels[2]))
print(paste0("Cluster 3 size: ", length(which(clusters==3)), ", label: ", cluster.labels[3]))
print(paste0("Cluster 4 size: ", length(which(clusters==4)), ", label: ", cluster.labels[4]))
print(paste0("Cluster 5 size: ", length(which(clusters==5)), ", label: ", cluster.labels[5]))
print(paste0("Observed purity: ", pur))
```


# SVM
In this section, we will use an SVM to try to predict subgroup. Unlike the previous two methods, SVM's are not easily adaptable to multiclass classification. So we will consider the simpler problem of distinguishing between the Luminal A and Luminal B.

```{r svm}
library(kernlab)

# Subsetting to luminal a and luminal b and splitting data. 
dat.sub <- cbind(dat.luma, dat.lumb)
labels.sub <- factor(c(rep(1, ncol(dat.luma)), rep(2, ncol(dat.lumb))))
train.inds.sub <- sample.int(length(labels.sub), round(0.8*length(labels.sub)))
dat.sub.train <- dat.sub[,train.inds.sub]
dat.sub.test <- dat.sub[,-train.inds.sub]
y.sub.train <- labels.sub[train.inds.sub]
y.sub.test <- labels.sub[-train.inds.sub]

# Fitting SVM.
svm.fit <- ksvm(t(dat.sub.train), y.sub.train)
train.pred <- predict(svm.fit, t(dat.sub.train))
test.pred <- predict(svm.fit, t(dat.sub.test))

# Get accuracy.
print("Luminal A vs. Luminal B")
print(paste0("Train accuracy: ", mean(train.pred == y.sub.train)))
print(paste0("Test accuracy: ", mean(test.pred == y.sub.test)))
print("")

# Subsetting to luminal a and luminal b and splitting data. 
dat.sub <- cbind(dat.her2, dat.basal)
labels.sub <- factor(c(rep(1, ncol(dat.her2)), rep(2, ncol(dat.basal))))
train.inds.sub <- sample.int(length(labels.sub), round(0.8*length(labels.sub)))
dat.sub.train <- dat.sub[,train.inds.sub]
dat.sub.test <- dat.sub[,-train.inds.sub]
y.sub.train <- labels.sub[train.inds.sub]
y.sub.test <- labels.sub[-train.inds.sub]

# Fitting SVM.
svm.fit <- ksvm(t(dat.sub.train), y.sub.train)
train.pred <- predict(svm.fit, t(dat.sub.train))
test.pred <- predict(svm.fit, t(dat.sub.test))

# Get accuracy.
print("Her2 vs. Basal")
print(paste0("Train accuracy: ", mean(train.pred == y.sub.train)))
print(paste0("Test accuracy: ", mean(test.pred == y.sub.test)))
print("")

# Subsetting to luminal a and luminal b and splitting data. 
dat.sub <- dat
labels.sub <- factor(c(rep(1, ncol(dat.luma)+ncol(dat.lumb)+ncol(dat.basal)+ncol(dat.her2)), 
                       rep(2, ncol(dat.normal))))
train.inds.sub <- sample.int(length(labels.sub), round(0.8*length(labels.sub)))
dat.sub.train <- dat.sub[,train.inds.sub]
dat.sub.test <- dat.sub[,-train.inds.sub]
y.sub.train <- labels.sub[train.inds.sub]
y.sub.test <- labels.sub[-train.inds.sub]

# Fitting SVM.
svm.fit <- ksvm(t(dat.sub.train), y.sub.train)
train.pred <- predict(svm.fit, t(dat.sub.train))
test.pred <- predict(svm.fit, t(dat.sub.test))

# Get accuracy.
print("Normal vs. Rest")
print(paste0("Train accuracy: ", mean(train.pred == y.sub.train)))
print(paste0("Test accuracy: ", mean(test.pred == y.sub.test)))
print("")
```