---
title: "COMP 755 Final Project"
author: "Kevin O'Connor"
date: "11/8/2018"
output:
  pdf_document: default
  html_document: default
---

In this document, we apply three different classification methods on The Cancer Genome Atlas (TCGA) Breast Cancer data. Specifically, we will use gene expression levels to predict breast cancer subtype which falls into one of five different groups: Luminal A, Luminal B, Basal, Normal, and HER2. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries and directories.
library(dplyr)
library(knitr)
library(R.utils)
library(dplyr)
library(limma)

sourceDirectory("/Users/kevinoconnor/Documents/Research/DCM/Differential-Correlation-Mining/DCM/R")
sourceDirectory("/Users/kevinoconnor/Documents/Research/DCM/Differential-Correlation-Mining/Utils")
data.dir <- file.path("/Users/kevinoconnor/Documents/Research/DCM/Data")
home.dir <- file.path("/Users/kevinoconnor/Documents/School/COMP_755")

# Reading data.
if(!(exists("SeqData") & exists("sampleTab"))){
  SeqData   <- read.delim(file.path(data.dir, "BRCA_1201_FINAL_Cluster.txt") , 
                          header=F)
  sampleTab <- read.delim(file.path(data.dir, "BRCA_1218_pam50scores_FINAL.txt"))
}
data   <- SeqData[-c(1:5),-(1:3)] 
SamBar <- SeqData[,-c(1:3)]
samTab <- filter(sampleTab, Use=="YES")
gene.names <- SeqData[-c(1:5), 1]
rownames(data) <- gene.names

# Match barcodes in both datasets.
SamBar0 <- unlist(lapply(SamBar[1,], as.character))
m       <- match(SamBar0, as.character(samTab$Barcode))
samTab0 <- samTab[m, ]
dataM0  <- apply(data, 2, as.numeric)
rownames(dataM0) <- as.vector(gene.names)

# Filter and transform data.
dataM <- filter_data(dataM0,
                     min.var=100,
                     max.var=10000)

# Create matrix for each group.
tar <- samTab0$Call
wb  <- which(tar=="Basal")
wla <- which(tar=="LumA")
wlb <- which(tar=="LumB")
wh <- which(tar=="Her2")
wn <- which(tar=="Normal")
MAM.basal  <- dataM[, wb]
MAM.LA     <- dataM[, wla]
MAM.LB     <- dataM[, wlb]
MAM.her2   <- dataM[, wh]
MAM.normal <- dataM[, wn]

# Standardize row within each group.
stdize <- function(gene){
  # Make zero mean.
  gene <- gene - mean(gene)
  
  # Make sum of squares one.
  gene <- gene/sqrt(sum(gene^2))
  
  return(gene)
}

MAM.LA.std <- MAM.LA
for(r in 1:nrow(MAM.LA)){
  MAM.LA.std[r,] <- stdize(MAM.LA[r,])
}

MAM.LB.std <- MAM.LB
for(r in 1:nrow(MAM.LB)){
  MAM.LB.std[r,] <- stdize(MAM.LB[r,])
}

MAM.basal.std <- MAM.basal
for(r in 1:nrow(MAM.basal)){
  MAM.basal.std[r,] <- stdize(MAM.basal[r,])
}

MAM.her2.std <- MAM.her2
for(r in 1:nrow(MAM.her2)){
  MAM.her2.std[r,] <- stdize(MAM.her2[r,])
}

MAM.normal.std <- MAM.normal
for(r in 1:nrow(MAM.normal)){
  MAM.normal.std[r,] <- stdize(MAM.normal[r,])
}

dat.luma   <- MAM.LA.std
dat.lumb   <- MAM.LB.std
dat.basal  <- MAM.basal.std
dat.her2   <- MAM.her2.std
dat.normal <- MAM.normal.std

write.table(dat.luma, file=file.path(home.dir, "luma.csv"), row.names=FALSE, col.names=FALSE, sep=",")
write.table(dat.lumb, file=file.path(home.dir, "lumb.csv"), row.names=FALSE, col.names=FALSE, sep=",")
write.table(dat.basal, file=file.path(home.dir, "basal.csv"), row.names=FALSE, col.names=FALSE, sep=",")
write.table(dat.her2, file=file.path(home.dir, "her2.csv"), row.names=FALSE, col.names=FALSE, sep=",")
write.table(dat.normal, file=file.path(home.dir, "normal.csv"), row.names=FALSE, col.names=FALSE, sep=",")
```

After reading in the data and removing genes with extremely small or large variance, we are left with p=`r nrow(dat.luma)` variables and sample sizes for each group given by

```{r sample_size}
kable(data.frame(group=c("Luminal A", "Luminal B", "Basal", "Normal", "HER2"), sample.size=c(ncol(dat.luma), ncol(dat.lumb), ncol(dat.basal), ncol(dat.normal), ncol(dat.her2))))
```

Yielding a total sample size of n=`r ncol(dat.luma)+ncol(dat.lumb)+ncol(dat.basal)+ncol(dat.normal)+ncol(dat.her2)`. Note that we have normalized the rows of each dataset.

Quickly, we will split the data up into a training and test sets.
```{r split_data}
dat <- cbind(dat.luma, dat.lumb, dat.basal, dat.her2, dat.normal)
labels <- c(rep(1, ncol(dat.luma)),
            rep(2, ncol(dat.lumb)),
            rep(3, ncol(dat.basal)),
            rep(4, ncol(dat.her2)),
            rep(5, ncol(dat.normal)))
n <- ncol(dat)
p <- nrow(dat)
train.inds <- sample.int(n, round(0.8*n))
dat.train <- dat[,train.inds]
dat.test <- dat[,-train.inds]
y.train <- labels[train.inds]
y.test <- labels[-train.inds]
```

# Multinomial Logistic Regression

First we will perform logistic regression to try and predict the subgroup which a given set of gene expression values belongs to. We can write our model as
\[ \mathbb{P}(y_i = k | X_i, \beta) = \frac{\exp\left\{-X_i \beta_k\right\}}{\sum_{k'} \exp\left\{-X_i \beta_{k'}\right\}}\]
Which gives a negative log-likelihood of
\[ \mathcal{NLL}(\beta) = \sum\limits_{i=1}^n -X_i \beta_k - \sum\limits_{i=1}^n \log\bigg(\sum\limits_{k'} \exp\left\{-X_i \beta_{k'}\right\}\bigg) \]

```{r neg_log_lkhd}
neg_log_likelihood <- function(beta, X){
  # Returns negative log-likelihood of the logistic regression model.
  #
  # Args:
  #  -beta: p by K matrix where the k'th column gives beta_k as in the equation
  #    above.
  #  -X: p by n data matrix.
  # Returns:
  #  A K by n matrix giving log-likelihood of each point and class.
  require(matrixStats)
  
  return(-t(beta) %*% X - sapply(1:ncol(beta), function(i){logSumExp(t(beta[,i]) %*% X)}))
}

get_log_likelihood_w_pen <- function(beta, X, y, pen_type="l1", lambda=0){
  # Returns penalized log-likelihood of the logistic regression model.
  #
  # Args:
  #  -beta: p by K matrix where the k'th column gives beta_k as in the equation
  #    above.
  #  -X: p by n data matrix.
  #  -y: n by 1 vector giving the labels of the data.
  #  -pen_type: Either "l1" or "l2" corresponding to the type of penalty term.
  #  -lambda: Penalty parameter.
  # Returns:
  #  A number corresponding to the penalized negative log-likelihood.
  
  nll.mat <- neg_log_likelihood(beta, X)
  # Now select only nll for correct classes.
  ind <- 1; nll <- c()
  for(label in y){
    nll <- c(nll, nll.mat[label, ind])
  }
  nll <- sum(nll)
  
  if(pen_type == "l1"){
    pen <- lambda*sum(abs(beta))
  } else if(pen_type == "l2"){
    pen <- lambda*sum(beta^2)
  } else {
    print("ERROR: Unknown penalty type!")
    return(NaN)
  }
  
  return(nll + pen)
}

# TODO: A test (?)
```

Next we will need an implementation of gradient descent to use for fitting.

```{r grad_desc}
compute_grad <- function(beta, X, y, pen_type="l1", lambda, delta=1e-4){
  # Initialize matrix of gradients.
  grad_mat <- beta
  for (i in nrow(beta)){
    for (j in ncol(beta)){
      l_beta <- r_beta <- beta
      l_beta[i,j] - delta/2
      r_beta[i,j] + delta/2
      grad_mat[i,j] <- (get_log_likelihood_w_pen(r_beta, X, y, pen_type, lambda) - get_log_likelihood_w_pen(l_beta, X, y, pen_type, lambda))/delta
    }
  }
  return(grad_mat)
}

grad_descent <- function(init_beta, X, y, pen_type="l1", lambda=0, delta=1e-4, learning_rate=1e-2, max.iter=10){
  beta <- init_beta
  
  for(i in 1:max.iter){
    if(i %% 1 == 0){print(paste0("Completed ", i, " iterations..."))}
    grad_mat <- compute_grad(beta, X, y, pen_type, lambda, delta)  
    
    beta <- beta - learning_rate*grad_mat
  }
  
  return(beta)
}

predict_class <- function(beta, X){
  # beta is p by K
  # X is p by n.
  
  predictions <- sapply(1:ncol(X), function(c){
    return(which.max(-t(beta) %*% X[,c]))
  })
  
  return(predictions)
}
```

Ok, now we can fit a multinomial logit model to our data.

```{r fit_multlogit}
init_beta <- matrix(c(0), nrow=p, ncol=n)
fitted.beta <- grad_descent(init_beta, dat.train, y, max.iter=10)
predicted_ys_train <- predict_class(fitted.beta, dat.train)
predicted_ys_test <- predict_class(fitted.beta, dat.test)
print(paste0("Train accuracy: ", mean(predicted_ys_train == y.train)))
print(paste0("Test accuracy: ", mean(predicted_ys_test == y.test)))
```

# Clustering
In this section, we will use an unsupervised algorithm to cluster the data. We hope that the clusters will correspond to subgroups of the samples. In order to measure the performance, we will compute the purity.

```{r kmeans}
cluster.kmeans <- kmeans(t(dat), 5)


```


# SVM
In this section, we will use an SVM to try to predict subgroup. Unlike te previous two methods, SVM's are not easily adaptable to multiclass classification. So we will consider the simpler problem of distinguishing between the Luminal A and Luminal B.

```{r svm}
library(kernlab)

# Subsetting to luminal a and luminal b and splitting data. 
dat.sub <- cbind(dat.luma, dat.lumb)
labels.sub <- factor(c(rep(1, ncol(dat.luma)), rep(2, ncol(dat.lumb))))
train.inds.sub <- sample.int(length(labels.sub), round(0.8*length(labels.sub)))
dat.sub.train <- dat.sub[,train.inds.sub]
dat.sub.test <- dat.sub[,-train.inds.sub]
y.sub.train <- labels.sub[train.inds.sub]
y.sub.test <- labels.sub[-train.inds.sub]

# Fitting SVM.
svm.fit <- ksvm(t(dat.sub.train), y.sub.train)
train.pred <- predict(svm.fit, t(dat.sub.train))
test.pred <- predict(svm.fit, t(dat.sub.test))

# Get accuracy.
print("Luminal A vs. Luminal B")
print(paste0("Train accuracy: ", mean(train.pred == y.sub.train)))
print(paste0("Test accuracy: ", mean(test.pred == y.sub.test)))
print("")

# Subsetting to luminal a and luminal b and splitting data. 
dat.sub <- cbind(dat.her2, dat.basal)
labels.sub <- factor(c(rep(1, ncol(dat.her2)), rep(2, ncol(dat.basal))))
train.inds.sub <- sample.int(length(labels.sub), round(0.8*length(labels.sub)))
dat.sub.train <- dat.sub[,train.inds.sub]
dat.sub.test <- dat.sub[,-train.inds.sub]
y.sub.train <- labels.sub[train.inds.sub]
y.sub.test <- labels.sub[-train.inds.sub]

# Fitting SVM.
svm.fit <- ksvm(t(dat.sub.train), y.sub.train)
train.pred <- predict(svm.fit, t(dat.sub.train))
test.pred <- predict(svm.fit, t(dat.sub.test))

# Get accuracy.
print("Her2 vs. Basal")
print(paste0("Train accuracy: ", mean(train.pred == y.sub.train)))
print(paste0("Test accuracy: ", mean(test.pred == y.sub.test)))
print("")

# Subsetting to luminal a and luminal b and splitting data. 
dat.sub <- dat
labels.sub <- factor(c(rep(1, ncol(dat.luma)+ncol(dat.lumb)+ncol(dat.basal)+ncol(dat.her2)), 
                       rep(2, ncol(dat.normal))))
train.inds.sub <- sample.int(length(labels.sub), round(0.8*length(labels.sub)))
dat.sub.train <- dat.sub[,train.inds.sub]
dat.sub.test <- dat.sub[,-train.inds.sub]
y.sub.train <- labels.sub[train.inds.sub]
y.sub.test <- labels.sub[-train.inds.sub]

# Fitting SVM.
svm.fit <- ksvm(t(dat.sub.train), y.sub.train)
train.pred <- predict(svm.fit, t(dat.sub.train))
test.pred <- predict(svm.fit, t(dat.sub.test))

# Get accuracy.
print("Normal vs. Rest")
print(paste0("Train accuracy: ", mean(train.pred == y.sub.train)))
print(paste0("Test accuracy: ", mean(test.pred == y.sub.test)))
print("")
```

