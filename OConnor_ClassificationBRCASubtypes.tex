\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Classification of Breast Cancer Subtypes via Gene Expression}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Kevin O'Connor  \\
  Department of Statistics and Operations Research\\
  University of North Carolina - Chapel Hill\\
  Chapel Hill, NC \\
  \texttt{koconn@live.unc.edu} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Breast cancer subtype identification is an important problem with high clinical relevance. In this paper we use various machine learning methods to predict patients' breast cancer subtype from the measured expression levels of a subset of their genes. It is shown that multinomial logistic regression can achieve a moderate level of accuracy whereas modern classification methods like svm and a fully-connected neural network can achieve far greater results. The performance of each method is then evaluated and compared.
\end{abstract}

\section{Introduction}
For many years now, successful treatments for various types of cancers have eluded re- searchers. Breast cancer in particular continues to affect millions of women and men. An important characteristic in the study of breast cancer is the subtype of disease, identified as Luminal A, Luminal B, Basal, HER2, or Normal. For a single tumor, the subtype determines both how the disease may develop in the patient as well as the proper course of treatment. Thus, there is significant incentive for researchers to find distinctive characteristics of each subtype to more clearly identify them in future patients.
\newline\newline In the past decade, the cost of obtaining genetic information from an individual has decreased substantially. This has led to a boom in the amount of genetic data available in many areas of medicine. In 2005, a database called The Cancer Genome Atlas (TCGA) was established in order to organize this data and make it widely available to researchers around the world for analysis. At this point, the database contains genetic expression levels for over 20,000 genes from patients with tumors in each of the 5 breast cancer subtypes. It offers an excellent opportunity to look for patterns in the genetic data that distinguish the subtypes.

\subsection{Code}
The code used to perform the analyses was written in R and Python/Keras and can be found at 
\begin{center}
\url{https://github.com/oconnor-kevin/comp755}
\end{center}

\subsection{Data}
The data can be downloaded from the following website,
\begin{center}
\url{https://portal.gdc.cancer.gov}
\end{center}

\section{Experiments}
\label{experiments}

\subsection{Multinomial Logistic Regression}
In our first experiment, we will fit a multinomial logistic regression model to the data. 
\subsubsection{The Model}
Given a sample $X_i \in \mathbb{R}^p$ with label $y_i \in \{1, ..., K\}$ and $K$ coefficient vectors, $\{\beta_1, ..., \beta_K\}$, we can write our model as
\[ \mathbb{P}(y_i = k) = \frac{\exp\left\{X_i\beta_k\right\}}{\sum_{k'} \exp\left\{X_i\beta_{k'}\right\}} \]
For $n$ independent observations, $X_1, ..., X_n$, this gives us a joint likelihood of the correct classes,
\[ \mathcal{L}(\beta_1, ..., \beta_K; \{X_i, y_i\}_{i=1}^n) = \prod\limits_{i=1}^n \frac{\exp\left\{X_i \beta_{y_i}\right\}}{\sum_{k'} \exp\left\{X_i \beta_{k'}\right\}} \]
and log-likelihood,
\[ \mathcal{NLL}(\beta_1, ..., \beta_K; \{X_i, y_i\}_{i=1}^n) = -\sum\limits_{i=1}^n\bigg[ X_i \beta_{y_i} + \log\bigg(\sum\limits_{k'} \exp\left\{X_i \beta_{k'}\right\}\bigg)\bigg] \]
\subsubsection{Training}
In order to fit the model to the data, we minimize the negative log-likelihood via gradient descent to find the maximum likelihood estimator of the parameters $\{\beta_1, ..., \beta_K\}$. Note that care has to be taken when computing the second term in the log-likelihood to avoid numerical overflow.

\subsection{K-means}
Next we consider an unsupervised learning approach to identify subgroups. We will apply the K-means algorithm to our data to produce 5 clusters which we hope will correspond to breast cancer subtypes. 
\subsubsection{The Algorithm}
In this algorithm, we start by randomly initializing the means of each cluster, $\{\hat{m}^0_1, ..., \hat{m}^0_5\}$. Then at iteration $t$, assign point $i$ to the cluster with the closest mean,
\[ \hat{y}_i^t = \mbox{argmin}_k \|X_i - \hat{m}^{t-1}_k\|_2 \]
recomputing the means of the newly clustered data after each iteration,
\[ \hat{m}^t_k = \frac{1}{n^t_k}\sum\limits_{i: \hat{y}_i^t = k} X_i \]
where $n_k^t$ is the number of points in cluster $k$ at iteration $t$. This is repeated until convergence or some maximum number of iterations is reached.
\subsubsection{Purity}
As this is an unsupervised clustering algorithm, the output won't specify which cluster corresponds to which subtype. If we did know this, it would be straightforward to compute the classification accuracy. But as this is not known, we need to think more carefully about how to evaluate the performance of the algorithm.
\newline\newline One option is to compute the purity. From a high level, the purity measures how homogeneous the clusters that have been found are. Specifically, let $n_{kl}$ be the number of elements in cluster $k$ which belong to subtype $l$. Then associate a label with cluster $k$,
\[ c_k = \mbox{argmax}_l n_{kl} \]
Then $c_k$ represents a majority vote of the labels in cluster $k$. Then define purity, $P$,
\[ P = \sum\limits_{k} \frac{n_{kc_k}}{n} \]
Notice that $P \in [0,1]$ and $P$ close to 1 corresponds to more homogeneous clusters. We will use this to evaluate the performance of the K-means algorithm.

\subsection{SVM}
Moving on to more modern classification methods, we apply an SVM. This attempts to find a boundary in the data space which maximizes the margin between the support vectors from different classes. This is achieved by minimizing the \textit{hinge loss}, 
\[ L_{\mbox{hinge}}(y, f(\mathbf{x})) = (1 - yf(\mathbf{x}))_+ \]
where $f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + w_0$, along with a penalty for the size of the weights, giving us a complete loss function of
\[ L(\mathbf{w}; \{X_i, y_i\}_{i=1}^n) = \sum\limits_{i=1}^n (1 - y_i (\mathbf{w}^TX_i + w_0))_+ + \frac{1}{2}\|\mathbf{w}\|^2_2 \]
which is minimized over $\mathbf{w}, w_0 \in \mathbb{R}^p, \mathbb{R}$. Unfortunately as SVM's partition the data space into only two regions, they do not generalize well to multi-class classification problems. In order to apply them to our problem, we try two approaches:
\begin{enumerate}
\item Consider a subset of the data with only two classes.
\item Consider a \textit{one vs rest} classification problem.
\end{enumerate}
The two pairs of subtypes considered are Luminal A vs. Luminal B and Her2 vs. Basal. In the one vs rest scheme, we consider predicting the Normal subtype vs. the other four.

\subsection{Fully-connected Neural Network}
While SVM's were for several decades considered to be state of the art in prediction problems, neural networks have surpassed them in the past decade. Many different kinds of neural networks have been developed for a variety of different prediction problems such as image classification, natural language processing, and time series learning. However, a simple fully-connected network will suffice for our data as there is no sequential or spatial dependencies between the data that might necessitate a more sophisticated network.
\newline\newline The network we will use has 4 densely connected hidden layers with sizes (1024, 2048, 1024, 512), batch normalization and dropout (with probability 0.5) at each layer, and ReLU activation functions. On the output layer, we have 5 nodes with a softmax activation. Furthermore, we train with 100 epochs and batch size of 20. The weights for the network can be found in the repository at the link in section 1.1.

\section{Discussion}
\begin{table}[t]
  \caption{Classification accuracy for held out test set with 20\% of data.}
  \label{class_acc}
  \centering
  \begin{tabular}{ll}
    \toprule
    \multicolumn{2}{c}{Classification Accuracy}                   \\
    \cmidrule{1-2}
    Model  & Accuracy  \\
    \midrule
     Mult. Logistic Regression & \\
	 FC Neural Network & 53.75\% \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[t]
  \caption{2 Class classification accuracy for held out test set with 20\% of data.}
  \label{class_acc_2}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{3}{c}{\textbf{SVM Classification Accuracy}}                   \\
    \cmidrule{1-3}
    \textbf{Subtypes}  & \textbf{Train Accuracy}  & \textbf{Test Accuracy}\\
    \midrule
     Luminal A vs. Luminal B & 99.53\% & 98.73\%\\
	 Her2 vs. Basal &  100.00\% & 96.36\%\\
	 Normal vs. Rest & 99.27\% & 90.83\%\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Conclusion}
In this document, we investigated a number of different classification methods for identifying breast cancer subtype using gene expression. We saw that even older methods like multinomial logistic regression and K-means were able to predict subtype with a moderate level accuracy. Furthermore, it was observed that the more modern methods like an SVM and a fully-connected neural network yielded an improvement in predictive performance as expected.

\subsubsection*{Acknowledgments}
Thank you to my girlfriend, Cambria, for giving up our Friday night together so I can finish this project!

\section*{References}

\small

[1] Murphy, Kevin P. "Machine Learning: A Probabilistic Perspective. Adaptive Computation and Machine Learning." (2012).

[2] Chollet, Francois. Deep learning with python. Manning Publications Co., 2017.

\end{document}

%% Tables
\begin{table}[t]
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule{1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}


%% Figures
\begin{figure}[h]
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}
